# 第3章 疱丁解Transformer

在第2章中，我们看到了微调和评估一个Transformer所需要的东西。 现在让我们来看看它们在引擎盖下是如何工作的。 在本章中，我们将探讨Transformer模型的主要构件以及如何使用PyTorch实现它们。 我们还将提供关于如何在TensorFlow中做同样事情的指导。 我们将首先专注于建立关注机制，然后添加必要的位和件，使Transformer编码器工作。 我们还将简单了解一下编码器和解码器模块之间的结构差异。 在本章结束时，你将能够自己实现一个简单的Transformer模型! 

虽然对Transformer架构有深刻的技术理解通常不是使用Transformer和为你的用例微调模型的必要条件，但对于理解和驾驭Transformer的限制以及在新领域使用它们是有帮助的。 

本章还介绍了Transformer的分类法，以帮助你了解近年来出现的各种型号的Transformer。 在深入研究代码之前，让我们先概述一下启动Transformer革命的原始架构。

## Transformers 架构

正如我们在第一章中所看到的，最初的Transformer是基于encoder-decoder架构的，该架构被广泛用于机器翻译等任务，即把一串词从一种语言翻译成另一种语言。 该架构由两个部分组成：

**编码器**

将输入的标记序列转换为嵌入向量序列，通常称为隐藏状态或上下文。 

**解码器** 

使用编码器的隐藏状态来迭代生成一个标记的输出序列，每次一个标记。 

如图3-1所示，编码器和解码器本身是由几个构件组成的：

![image-20220213101256274](images/chapter3/image-20220213101256274.png)

我们很快就会看到每个组件的细节，但我们已经可以在图3-1中看到一些描述Transformer架构的东西：

