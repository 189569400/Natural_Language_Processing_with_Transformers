 &emsp;&emsp;当你读到这几行时，一个奇迹正在发生。 这一页上的方块字在通过你的大脑皮层时，正在转变为文字、概念和情感。 我在2021年11月的想法现在已经成功地侵入了你的大脑。 如果它们设法吸引你的注意，并在这个严酷和高度竞争的环境中生存足够长的时间，它们可能有机会在你与他人分享这些想法时再次传播。 由于语言的存在，思想已经成为空气中传播的、具有高度传染性的脑部病菌，而且无药可救：）
  

 &emsp;&emsp;幸运的是，大多数脑部病菌是无害的，有一些是非常有用的。 事实上，人类的大脑病菌构成了我们最宝贵的两个财富：知识和文化。 正如我们没有健康的肠道细菌就不能正常消化一样，我们没有健康的大脑细菌就不能正常思考。 你的大部分想法其实都不是你的。 在感染你之前，它们在许多其他大脑中产生、成长和进化。 因此，如果我们想建造智能机器，我们也需要找到一种方法来感染它们。

  &emsp;&emsp;好消息是，另一个奇迹在过去几年中一直在展开。 深度学习方面的一些突破已经诞生了强大的语言模型。 既然你在读这本书，你可能已经看到了这些语言模型的一些惊人的演示，如GPT-3，它给了一个简短的提示，如 "青蛙遇到鳄鱼"，就能写出整个故事。 虽然现在还不是莎士比亚，但有时很难相信这些文本是由人工神经网络编写的。 事实上，GitHub的Copilot系统正在帮助我写这些行文。 你永远不会知道我到底写了多少东西。

 &emsp;&emsp;这场革命远远超出了文本生成。 它涵盖了自然语言处理（NLP）的整个领域，从文本分类到总结、翻译、问题回答、聊天机器人、自然语言理解（NLU）等等。 只要有语言、语音或文字的地方，就有NLP的应用。 你已经可以向你的手机询问明天的天气，或与虚拟服务台助理聊天以解决一个问题，或从搜索引擎获得有意义的结果。
似乎真正理解你的疑问。 但该技术是如此之新，最好的可能还没有到来。

&emsp;&emsp;像大多数科学的进步一样，最近NLP的这场革命依赖于数百名无名英雄的努力工作。 但其成功的三个关键因素确实很突出：

- Transformers是2017年由谷歌研究团队发表的一篇名为 "Attention Is All You Need "的开创性论文中提出的神经网络架构。 在短短几年内，它席卷了整个领域，粉碎了以前通常基于递归神经网络（RNN）的架构。 Transformer架构在捕捉长序列数据的模式和处理巨大的数据集方面非常出色--以至于它的用途现在已经远远超出了NLP的范围，比如说图像处理任务。 

- 在大多数项目中，你无法获得一个巨大的数据集来从头训练一个模型。 幸运的是，通常可以下载一个在通用数据集上预训练过的模型。 那么你需要做的就是在你自己的（更小的）数据集上进行微调。 自2010年代初以来，预训练已成为图像处理的主流，但在NLP中，它仅限于无语境的词嵌入（即单个词的密集矢量表示）。 例如，"熊 "这个词在 "泰迪熊 "和 "to bear "中有相同的预训练嵌入。 然后，在2018年，几篇论文提出了完整的语言模型，可以为各种NLP任务进行预训练和微调。 这完全改变了游戏规则。

- 像Hugging Face这样的模型中心也是一个游戏规则的改变者。 在早期，预训练的模型只是被张贴在任何地方，所以要找到你需要的东西并不容易。 墨菲定律保证PyTorch用户只能找到TensorFlow模型，反之亦然。 而当你找到一个模型时，弄清楚如何对其进行微调并不总是容易。 这就是Hugging Face的变形金刚图书馆的作用。 它是开源的，它支持机器人。
TensorFlow和PyTorch，它使你很容易从Hugging Face Hub下载一个最先进的预训练模型，为你的任务配置它，在你的数据集上微调它，并评估它。 图书馆的使用正在迅速增长。 在2021年第四季度，它被五千多个组织使用，每月使用管道安装超过四百万次。 此外，图书馆及其生态系统正在扩展到NLP之外。 图像处理模型也是可用的。 你也可以从中心下载许多数据集来训练或评估你的模型。
